{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare PDF RAG strategies\n",
    "This notebook demonstrates how to evaluate retrieval augmented generation using text extracted from PDFs either via direct metadata extraction or with help from Mistral. Azure OpenAI provides the language models for answering questions and scoring results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from compare_pdf_rag import (\n",
    "    extract_metadata_text, extract_mistral_text,\n",
    "    build_vector_store, answer_questions, evaluate_rag\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = Path('example.pdf')  # path to your PDF\n",
    "questions_path = Path('questions.txt')\n",
    "truths_path = Path('answers.txt')\n",
    "method = 'metadata'  # or 'mistral'\n",
    "mistral_api_key = 'YOUR_MISTRAL_API_KEY'  # only needed for Mistral method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if method == 'metadata':\n",
    "    text = extract_metadata_text(pdf_path)\n",
    "else:\n",
    "    text = extract_mistral_text(pdf_path, api_key=mistral_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = build_vector_store(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [q.strip() for q in questions_path.read_text().splitlines() if q.strip()]\n",
    "truths = [t.strip() for t in truths_path.read_text().splitlines() if t.strip()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = answer_questions(store, questions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = evaluate_rag(outputs, truths)\n",
    "scores\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
